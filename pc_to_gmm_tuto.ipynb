{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud Modeling using Self-Organizing Gaussian Mixture Models (SOGMM)\n",
    "This tutorial presents modeling a point cloud using the SOGMM approach presented in [1].\n",
    "We assume that a depth-intensity point cloud with $N$ points (i.e., point cloud is a set of\n",
    "$N$ four-dimensional points $\\mathbf{x}$) is given. The output is a Gaussian Mixture Model (GMM)\n",
    "of the point cloud that can be used for:\n",
    "1. Inference for intensity image reconstruction\n",
    "2. Dense sampling for 3D point cloud reconstruction \n",
    "\n",
    "## Input\n",
    "For the purposes of this tutorial, download the point cloud data from\n",
    "[this](https://cmu.box.com/s/9v7v1knt9iswui2518c6qpqtzdqnf2v5) link.  The\n",
    "downloaded zip folder contains point cloud data from a few frames of the\n",
    "`stonewall`, `copyroom`, and `lounge` ICL-NUIM datasets [2].\n",
    "\n",
    "Extract the zip folder. You will get a folder with files named like this:\n",
    "\n",
    "```\n",
    "gira3d-tutorial-data\n",
    "├── pcd_copyroom_2364_decimate_1_0.pcd\n",
    "├── pcd_copyroom_2364_decimate_2_0.pcd\n",
    "└── ...\n",
    "```\n",
    "The files are named as `pcd_<dataset-name>_<frame-number>_decimate_<decimation_amount>.pcd`.\n",
    "Here, decimation of 1.0 means the original image of size `640 x 480` was used to generate the point cloud.\n",
    "Likewise, decimations of `2.0`, `3.0`, and `4.0` correspond to image sizes `320 x 240`, `213 x 160`, and\n",
    "`160 x 120` respectively.\n",
    "\n",
    "For the purposes of this tutorial, we limit to the frames provided in the folder above. However, the\n",
    "methodology will work on any registered depth-intensity image pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning the SOGMM Model\n",
    "We start by importing the run time dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "from sogmm_py.utils import read_log_trajectory, o3d_to_np, np_to_o3d\n",
    "from sogmm_py.vis_open3d import VisOpen3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, we will use frame `1763` of the `lounge` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = 935\n",
    "# datasetname = 'copyroom'\n",
    "\n",
    "# frame = 103\n",
    "# datasetname = 'stonewall'\n",
    "\n",
    "frame = 1763\n",
    "datasetname = 'lounge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a ground truth point cloud from the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcld_gt = o3d.io.read_point_cloud('./gira3d-tutorial-data/pcd_' +\n",
    "                                  str(datasetname) +\n",
    "                                  '_' + str(frame) +\n",
    "                                  '_decimate_1.pcd', format='pcd')\n",
    "pcld_gt_np = o3d_to_np(pcld_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216035, 4)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcld_gt_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the ground truth SE(3) pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = read_log_trajectory('./gira3d-tutorial-data/' +\n",
    "                           str(datasetname) + '-traj.log')\n",
    "pcld_pose = traj[frame].pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the intrinsics matrix and width and height variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.eye(3)\n",
    "K[0, 0] = 525.0\n",
    "K[1, 1] = 525.0\n",
    "K[0, 2] = 319.5\n",
    "K[1, 2] = 239.5\n",
    "\n",
    "W = (int)(640)\n",
    "H = (int)(480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize, please use our python wrapper over the Open3D visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = VisOpen3D(visible=True)\n",
    "vis.visualize_pcld(pcld_gt, pcld_pose, K, W, H)\n",
    "vis.render()\n",
    "del vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an interactive Open3D window that shows the depth-intensity point cloud and the camera frustrum:\n",
    "![Ground Truth Point Cloud](results/sogmm-tut-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the SOGMM method produces a generative model of the point cloud, we can train it on decimated point clouds. In this example, let us use a decimation of `4.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "deci = 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the decimated point cloud from the dataset folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcld_deci = o3d.io.read_point_cloud('./gira3d-tutorial-data/pcd_' +\n",
    "                                  str(datasetname) +\n",
    "                                  '_' + str(frame) +\n",
    "                                  '_decimate_' +\n",
    "                                  str(deci).replace('.', '_') +\n",
    "                                  '.pcd', format='pcd')\n",
    "pcld_deci_np = o3d_to_np(pcld_deci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the intrinsics matrix and width and height variables corresponding to the decimation factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_d = np.eye(3)\n",
    "K_d[0, 0] = 525.0/deci\n",
    "K_d[1, 1] = 525.0/deci\n",
    "K_d[0, 2] = 319.5/deci\n",
    "K_d[1, 2] = 239.5/deci\n",
    "\n",
    "W_d = (int)(640/deci)\n",
    "H_d = (int)(480/deci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the decimated point cloud the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = VisOpen3D(visible=True)\n",
    "vis.visualize_pcld(pcld_deci, pcld_pose, K_d, W_d, H_d)\n",
    "vis.render()\n",
    "del vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decimated point cloud looks like this:\n",
    "![Decimated Point Cloud](results/sogmm-tut-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we learn a SOGMM on the decimated point cloud. Both CPU-only and GPU-accelerated cases are supported. Both of these modules can be accessed through the `SOGMM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogmm_py.sogmm import SOGMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOGMM CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute platform is CPU\n"
     ]
    }
   ],
   "source": [
    "sg_cpu = SOGMM(bandwidth=0.02, compute='CPU')\n",
    "model_cpu = sg_cpu.fit(pcld_deci_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the number of components automatically chosen by SOGMM for this frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gaussians_with_camera(model, camera_pose, K, W, H, scale=2.0):\n",
    "    \"\"\"\n",
    "    Visualize Gaussians with camera position and frustum\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The SOGMM model\n",
    "    - camera_pose: 4x4 SE(3) transformation matrix for camera position\n",
    "    - K: 3x3 Camera intrinsics matrix\n",
    "    - W: Image width\n",
    "    - H: Image height \n",
    "    - scale: Base scale factor for ellipsoids\n",
    "    \"\"\"\n",
    "    import open3d as o3d\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create visualization\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    \n",
    "    # Add camera frustum visualization\n",
    "    # Create camera visualization using K, W, H\n",
    "    # Using camera intrinsics to create frustum\n",
    "    fx, fy = K[0, 0], K[1, 1]\n",
    "    cx, cy = K[0, 2], K[1, 2]\n",
    "    \n",
    "    # Create a simple camera frustum\n",
    "    camera_mesh = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5)\n",
    "    \n",
    "    # Create frustum lines\n",
    "    points = []\n",
    "    lines = []\n",
    "    colors = []\n",
    "    \n",
    "    # Camera center\n",
    "    camera_center = np.array([0, 0, 0])\n",
    "    points.append(camera_center)\n",
    "    \n",
    "    # Define the corners of the image plane at z=1\n",
    "    z = 1.0\n",
    "    top_left = np.array([(0 - cx) * z / fx, (0 - cy) * z / fy, z])\n",
    "    top_right = np.array([(W - cx) * z / fx, (0 - cy) * z / fy, z])\n",
    "    bottom_left = np.array([(0 - cx) * z / fx, (H - cy) * z / fy, z])\n",
    "    bottom_right = np.array([(W - cx) * z / fx, (H - cy) * z / fy, z])\n",
    "    \n",
    "    # Scale frustum for better visibility\n",
    "    frustum_scale = 0.3\n",
    "    top_left *= frustum_scale\n",
    "    top_right *= frustum_scale\n",
    "    bottom_left *= frustum_scale\n",
    "    bottom_right *= frustum_scale\n",
    "    \n",
    "    # Add corners\n",
    "    points.extend([top_left, top_right, bottom_left, bottom_right])\n",
    "    \n",
    "    # Add lines from center to corners\n",
    "    lines.extend([[0, 1], [0, 2], [0, 3], [0, 4]])\n",
    "    \n",
    "    # Add lines around the image plane\n",
    "    lines.extend([[1, 2], [2, 4], [4, 3], [3, 1]])\n",
    "    \n",
    "    # Set a distinct color for frustum\n",
    "    frustum_color = [1, 0.5, 0]  # Orange\n",
    "    colors = [frustum_color for _ in range(len(lines))]\n",
    "    \n",
    "    # Create line set for frustum\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Transform the camera according to pose\n",
    "    line_set.transform(camera_pose)\n",
    "    camera_mesh.transform(camera_pose)\n",
    "    \n",
    "    vis.add_geometry(line_set)\n",
    "    vis.add_geometry(camera_mesh)\n",
    "    \n",
    "    # For each Gaussian component\n",
    "    for i in range(len(model.means_)):\n",
    "        mean = model.means_[i, :3]\n",
    "        intensity = model.means_[i, 3]\n",
    "        \n",
    "        try:\n",
    "            # Reshape to 4x4 and extract 3x3 spatial part\n",
    "            cov_flat = model.covariances_[i]\n",
    "            cov_4x4 = cov_flat.reshape(4, 4)\n",
    "            cov = cov_4x4[:3, :3]\n",
    "            \n",
    "            # Create ellipsoid\n",
    "            mesh = o3d.geometry.TriangleMesh.create_sphere()\n",
    "            \n",
    "            # Calculate eigenvalues and eigenvectors\n",
    "            eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "            \n",
    "            # Scale by eigenvalues\n",
    "            component_scale = scale\n",
    "            \n",
    "            transform = np.eye(4)\n",
    "            transform[:3, :3] = np.dot(eigenvectors, np.diag(np.sqrt(np.abs(eigenvalues)) * component_scale))\n",
    "            transform[:3, 3] = mean\n",
    "            mesh.transform(transform)\n",
    "            \n",
    "            # Set color based on intensity\n",
    "            gray_val = intensity  # Already in 0-1 range\n",
    "            mesh.paint_uniform_color([gray_val, gray_val, gray_val])\n",
    "            \n",
    "            vis.add_geometry(mesh)\n",
    "            \n",
    "        except (np.linalg.LinAlgError, ValueError) as e:\n",
    "            print(f\"Error with component {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Set rendering options\n",
    "    render_option = vis.get_render_option()\n",
    "    render_option.background_color = np.array([0.1, 0.1, 0.1])  # Dark background\n",
    "    render_option.light_on = True\n",
    "    \n",
    "    # Set the viewpoint to the camera position\n",
    "    view_control = vis.get_view_control()\n",
    "    camera_position = camera_pose[:3, 3]\n",
    "    # Look camera position minus the camera direction\n",
    "    view_control.set_lookat(camera_position + 1.5*camera_pose[:3, 2])  # Z-axis of the camera\n",
    "    view_control.set_up(-camera_pose[:3, 1])  # Y-axis of the camera\n",
    "    view_control.set_front(-camera_pose[:3, 2])  # Negative Z-axis of the camera\n",
    "    \n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "\n",
    "visualize_gaussians_with_camera(model_cpu, pcld_pose, K_d, W_d, H_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cpu.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOGMM GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute platform is GPU\n"
     ]
    }
   ],
   "source": [
    "sg_gpu = SOGMM(bandwidth=0.02, compute='GPU')\n",
    "model_gpu = sg_gpu.fit(pcld_deci_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gaussians_with_camera(model_gpu, pcld_pose, K_d, W_d, H_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure we got the same number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpu.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for Intensity Image\n",
    "Intensity image can be constructed using the conditional GMM as follows. For more details on how the conditional distribution is computed, please refer to [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.9 s, sys: 850 ms, total: 46.7 s\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_, expected_intensities, _ = model_gpu.color_conditional(pcld_gt_np[:, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`expected_intensities` contains the expected intensity values at the ground truth 3D points. Let us visualize these reconstructed intensity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_pcld = np.zeros(pcld_gt_np.shape)\n",
    "recon_pcld[:, 0:3] = pcld_gt_np[:, 0:3] # we are constructing intensity image on gt 3D points\n",
    "recon_pcld[:, 3] = np.squeeze(expected_intensities)\n",
    "\n",
    "vis = VisOpen3D(visible=True)\n",
    "vis.visualize_pcld(np_to_o3d(recon_pcld), pcld_pose, K, W, H)\n",
    "vis.render()\n",
    "del vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reconstructed Point Cloud](results/sogmm-tut-3.png)\n",
    "\n",
    "Higher accuracy can be achieved if the learning is performed at a lower `deci` value (at the cost of higher computation time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense sampling for 3D point cloud reconstruction\n",
    "Dense sampling from the 4D GMM can be performed using the usual Box-Mueller sampling method [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_pcld = sg_gpu.joint_dist_sample(pcld_gt_np.shape[0]) # sample 4D points from the model\n",
    "vis = VisOpen3D(visible=True)\n",
    "vis.visualize_pcld(np_to_o3d(resampled_pcld), pcld_pose, K, W, H)\n",
    "vis.render()\n",
    "del vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Resampled Point Cloud](results/sogmm-tut-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures\n",
    "We support computing the following:\n",
    "1. Peak Signal-To-Noise Ratio (PSNR): To measure the accuracy of intensity image reconstruction.\n",
    "2. Structure Similarity Index Measure (SSIM): To measure the accuracy of intensity image reconstruction.\n",
    "3. F-score: To measure the accuracy of resampled point cloud.\n",
    "4. Precision: To measure the accuracy of resampled point cloud.\n",
    "5. Recall: To measure the accuracy of resampled point cloud.\n",
    "6. Mean Reconstruction Error (MRE): To measure the accuracy of resampled point cloud.\n",
    "7. Std. Dev. Reconstruction Error: To measure the accuracy of resampled point cloud.\n",
    "8. Memory: To measure the compactness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fscore 0.979307 precision 0.993742 recall 0.965286 recon. mean 0.003423 recon. std. dev. 0.003788\n",
      "psnr 32.009320 ssim 0.825107\n"
     ]
    }
   ],
   "source": [
    "from sogmm_py.utils import calculate_depth_metrics, calculate_color_metrics\n",
    "fsc, pre, re, rm, rs = calculate_depth_metrics(pcld_gt, np_to_o3d(resampled_pcld))\n",
    "print(\"fscore %f precision %f recall %f recon. mean %f recon. std. dev. %f\" % (fsc, pre, re, rm, rs))\n",
    "\n",
    "from sogmm_py.utils import ImageUtils\n",
    "iu = ImageUtils(K) # image manipulation utility\n",
    "_, gt_g = iu.pcld_wf_to_imgs(pcld_pose, pcld_gt_np) # project gt pcld on camera\n",
    "if np.isnan(gt_g).any():\n",
    "    gt_g = np.nan_to_num(gt_g)\n",
    "_, pr_g = iu.pcld_wf_to_imgs(pcld_pose, recon_pcld) # project recon pcld on camera\n",
    "if np.isnan(pr_g).any():\n",
    "    pr_g = np.nan_to_num(pr_g)\n",
    "psnr, ssim = calculate_color_metrics(gt_g, pr_g) # compare the intensity images\n",
    "print(\"psnr %f ssim %f\" % (psnr, ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory 35280 bytes\n"
     ]
    }
   ],
   "source": [
    "# computing memory usage\n",
    "M = model_gpu.n_components_\n",
    "mem_bytes = 4 * M * (1 + 10 + 4)\n",
    "print('memory %d bytes' % (mem_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] K. Goel, N. Michael, and W. Tabib, “Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture Models,” IEEE Robotics and Automation Letters, vol. 8, no. 5, pp. 2526–2533, May 2023, doi: 10.1109/LRA.2023.3256923.\n",
    "\n",
    "[2] Q.-Y. Zhou and V. Koltun, “Dense scene reconstruction with points of interest,” ACM Trans. Graph., vol. 32, no. 4, pp. 1–8, Jul. 2013, doi: 10.1145/2461912.2461919.\n",
    "\n",
    "[3] C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine\n",
    "learning. Springer, 2006, vol. 4, no. 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
